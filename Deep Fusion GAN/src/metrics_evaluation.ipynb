{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupied-findings",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quiet-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "needed-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "primary-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import linalg\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.stats import entropy\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sample import prepare_data, generate_images\n",
    "from src.generator.model import Generator\n",
    "from src.text_encoder.model import RNNEncoder\n",
    "from utils import create_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naval-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-summer",
   "metadata": {},
   "source": [
    "# Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excited-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True).to(self.device)\n",
    "        print(self.model.fc)\n",
    "        self.linear = self.model.fc\n",
    "        self.model.fc, self.model.dropout = [nn.Sequential()] * 2\n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def get_last_layer(self, x):\n",
    "        x = F.interpolate(x, size=300, mode='bilinear', align_corners=False, recompute_scale_factor=False)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metropolitan-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Dharneeshkar J/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "classifier = InceptionV3().to(device)\n",
    "classifier = classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-tooth",
   "metadata": {},
   "source": [
    "# Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mounted-merchant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames: 11788\n",
      "Load captions from: ../data\\captions.pickle\n",
      "Load file names from: ../data\\test\\filenames.pickle (2933)\n",
      "Dictionary size: 5450\n",
      "Embeddings number: 10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_loader = create_loader(256, batch_size, \"../data\", \"test\")\n",
    "n_words = test_loader.dataset.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-ceiling",
   "metadata": {},
   "source": [
    "# Generator + Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enclosed-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "generator.load_state_dict(torch.load(\"../gen_weights/gen_epoch_310.pth\", map_location=device))\n",
    "generator = generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "green-mortality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\spring 22\\deep learning\\final_project\\deep_fusion_gan-main\\venv\\cnaps\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "text_encoder = RNNEncoder.load(\"../text_encoder_weights/text_encoder200.pth\", n_words)\n",
    "text_encoder.to(device)\n",
    "\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "text_encoder = text_encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-turtle",
   "metadata": {},
   "source": [
    "# FID calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hawaiian-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(repr1, repr2):\n",
    "    # shape of reprs: (-1, embed_dim)\n",
    "    \n",
    "    # shape of mus: (embed_dim, )\n",
    "    mu_r, mu_g = np.mean(repr1, axis=0), np.mean(repr2, axis=0)\n",
    "    # rowvar=False:\n",
    "    #     each column represents a variable, while the rows contain observations\n",
    "    # shape of sigmas: (embed_dim, embed_dim)\n",
    "    sigma_r, sigma_g = np.cov(repr1, rowvar=False), np.cov(repr2, rowvar=False)\n",
    "    \n",
    "    diff = mu_r - mu_g\n",
    "    diff_square_norm = diff.dot(diff)\n",
    "    \n",
    "    product = sigma_r.dot(sigma_g)\n",
    "    sqrt_product, _ = sqrtm(product, disp=False)\n",
    "    \n",
    "    # np.isfinite:\n",
    "    #     Test element-wise for finiteness \n",
    "    #     (not infinity and not Not a Number)\n",
    "    if not np.isfinite(sqrt_product).all():\n",
    "        eye_matrix = np.eye(sigma_r.shape[0]) * 1e-8\n",
    "        sqrt_product = linalg.sqrtm((sigma_r + eye_matrix).dot(sigma_g + eye_matrix))\n",
    "    \n",
    "    # np.iscomplexobj:\n",
    "    #     Check for a complex type or an array of complex numbers.\n",
    "    #     The return value, True if x is of a complex type\n",
    "    #     or has at least one complex element.\n",
    "    if np.iscomplexobj(sqrt_product):\n",
    "        sqrt_product = sqrt_product.real\n",
    "\n",
    "    fid = diff_square_norm + np.trace(sigma_r + sigma_g - 2 * sqrt_product)\n",
    "    \n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "younger-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_representations():\n",
    "    real_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "    fake_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Build representations\")):\n",
    "        images, captions, captions_len, file_names = prepare_data(batch, device)\n",
    "        sent_emb = text_encoder(captions, captions_len).detach()\n",
    "\n",
    "        fake_images = generate_images(generator, sent_emb, device)\n",
    "\n",
    "        clf_out_real = classifier.get_last_layer(images)\n",
    "        clf_out_fake = classifier.get_last_layer(fake_images)\n",
    "\n",
    "\n",
    "        real_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_real\n",
    "        fake_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_fake\n",
    "            \n",
    "    return real_reprs, fake_reprs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-display",
   "metadata": {},
   "source": [
    "## Build representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educated-simpson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82f615eabdf5429595cb771436cfc45d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "real_values, fake_values = build_representations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-process",
   "metadata": {},
   "source": [
    "## FID value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "contrary-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 17.588754278680476\n"
     ]
    }
   ],
   "source": [
    "fid_value = calculate_fid(real_values, fake_values)\n",
    "print(f\"FID value = {fid_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-attention",
   "metadata": {},
   "source": [
    "# Inception score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ultimate-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_score(reprs, batch_size):\n",
    "    def get_pred(x):\n",
    "        x = classifier.linear(torch.tensor(x, dtype=torch.float))\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "\n",
    "    preds = np.zeros((reprs.shape[0], 1000))\n",
    "    \n",
    "    splits = 0\n",
    "    for i in range(0, len(preds), batch_size):\n",
    "        z = get_pred(reprs[i:i + batch_size])\n",
    "        preds[i:i + batch_size] = z\n",
    "        splits += 1\n",
    "    \n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * batch_size: (k+1) * batch_size, :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "            \n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "instrumental-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dharneeshkar J\\AppData\\Local\\Temp\\ipykernel_21920\\2009555802.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x).data.cpu().numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": "(4.428573218348446, 0.5791097173115292)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception_score(fake_values, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(2912, 2048)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_values.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}